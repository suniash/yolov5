{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOKs4ksbdyc2xozhsaPvRoI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Enhanced Road Traffic Sign Detection with YOLOv5**\n","\n","This Jupyter Notebook demonstrates the application of YOLOv5, a leading deep learning model, for object detection tasks, specifically focusing on a road sign dataset. YOLOv5 is celebrated for its efficiency and accuracy in identifying objects within images across various contexts. The primary aim to adapt, leverage YOLOv5 model's pre-existing knowledge and fine-tune it on the road sign dataset to enhance its detection capabilities to recognize and classify various road signs. This notebook encompasses the entire workflow, including data preparation, model configuration, training, and evaluation, offering an in-depth exploration of utilizing YOLOv5 for practical object detection challenges.This comprehensive approach aims to demonstrate the effectiveness of YOLOv5 in handling the nuanced task of road sign detection, a critical component for autonomous driving systems and traffic management.\n","\n","## **Setting Up the YOLOv5 Environment**\n","\n","In this section, we initialize our project environment by cloning the official YOLOv5 repository from GitHub. This step ensures we have the latest YOLOv5 framework and its dependencies. After cloning, we navigate into the yolov5 directory and reset the repository to a specific commit to ensure consistency and compatibility with our dataset and training procedure."],"metadata":{"id":"7AMpdNP8w3dq"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"cT-m-ZkkZyY0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707263470865,"user_tz":480,"elapsed":1184,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"}},"outputId":"9d0d8594-26f9-4c47-869d-254b46c661ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 16413, done.\u001b[K\n","remote: Counting objects: 100% (5/5), done.\u001b[K\n","remote: Compressing objects: 100% (5/5), done.\u001b[K\n","remote: Total 16413 (delta 0), reused 0 (delta 0), pack-reused 16408\u001b[K\n","Receiving objects: 100% (16413/16413), 14.95 MiB | 26.71 MiB/s, done.\n","Resolving deltas: 100% (11260/11260), done.\n","/content/yolov5/yolov5\n","HEAD is now at 064365d8 Update parse_opt() in export.py to work as in train.py (#10789)\n"]}],"source":["!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","!git reset --hard 064365d8683fd002e9ad789c1e91fa3d021b44f0"]},{"cell_type":"markdown","source":["## **Importing Necessary Libraries and Modules**\n","\n","Before diving into the model training and data processing, we import essential Python libraries and modules."],"metadata":{"id":"SgbR6nFKwdiI"}},{"cell_type":"code","source":["import glob\n","import yaml\n","\n","import torch\n","from IPython.display import Image, display, clear_output\n","from google.colab import drive\n","\n","from utils.downloads import attempt_download\n","from utils.plots import plot_results\n","from IPython.core.magic import register_line_cell_magic\n"],"metadata":{"id":"Dq8nkoiIbA6X","executionInfo":{"status":"ok","timestamp":1707263882738,"user_tz":480,"elapsed":267,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["## **Installing Dependencies and Verifying Setup**\n","\n","This section focuses on installing all the Python packages required by YOLOv5 to function correctly, ensuring our environment is correctly set up. Check whether a CUDA-enabled GPU is available for training and inference tasks, providing a clear indication of the computational resources available to us (GPU or CPU)"],"metadata":{"id":"wmheQbsr3XIt"}},{"cell_type":"code","source":["!pip install -qr requirements.txt\n","\n","# clear_output()\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"metadata":{"id":"hMJulp932F3u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Integrating Roboflow for Dataset Management**\n","\n","In this step, we introduce Roboflow, a powerful tool for managing and versioning datasets, by installing its Python package. Roboflow facilitates the preprocessing, augmentation, and distribution of datasets, making it an invaluable resource for ML projects. After installing the Roboflow package, we initialize the Roboflow client with our API key. This connection allows us to access our specific project within Roboflow's workspace."],"metadata":{"id":"IfOfjPiQ4YeB"}},{"cell_type":"code","source":["!pip install -q roboflow\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"YOUR API KEY\")\n","project = rf.workspace(\"sit-asmsw\").project(\"road-sign-detection-in-real-time\")\n","dataset = project.version(3).download(\"yolov5\")"],"metadata":{"id":"ANhp_QEqg_nx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Loading the Dataset Configuration with YAML**\n","After setting the current working directory, where our YOLOv5 environment is prepared, we proceed to examine the dataset configuration. The YAML file, automatically generated by Roboflow, specifies paths to the training, validation, and test images, as well as the class names and number of classes.\n","\n","This step ensures we have a clear overview of our dataset's structure and confirms that the dataset is correctly set up for the subsequent training process."],"metadata":{"id":"E5rjWYHp-4ev"}},{"cell_type":"code","source":["%cd /content/yolov5\n","\n","# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n","%cat {dataset.location}/data.yaml"],"metadata":{"id":"Y5tk5baehNmJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## **Defining the Number of Classes from the Dataset**\n","\n","By reading the data.yaml file, which was previously confirmed to contain paths and class information, we retrieve the number of classes (nc) defined within. This is a critical step in dynamically adjusting our model to accurately reflect the dataset it will be trained on, ensuring consistency and correctness in the model's output layer to match the number of target classes.\n","\n","This process not only automates the setup, making it more robust and less prone to manual errors but also enhances the notebook's adaptability to different datasets with varying numbers of classes."],"metadata":{"id":"EWc_mbcA_i1g"}},{"cell_type":"code","source":["# define number of classes based on YAML\n","with open(dataset.location + \"/data.yaml\", 'r') as stream:\n","    num_classes = str(yaml.safe_load(stream)['nc'])"],"metadata":{"id":"e5Zw5AcAhYjv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Reviewing the YOLOv5 Model Configuration**\n","\n","Reviewing the model configuration helps ensure that the model aligns with our project's objectives and provides an opportunity to make any necessary adjustments, such as changing the number of filters in the final layer to match the number of classes in our dataset. This alignment is crucial for the model's ability to accurately detect and classify objects within our specific domain."],"metadata":{"id":"Rpc1jSM7BDs6"}},{"cell_type":"code","source":["#this is the model configuration\n","%cat /content/yolov5/models/yolov5s.yaml"],"metadata":{"id":"4zXliNFDhbl_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Customizing IPython Magic for File Writing**\n","\n","This function is particularly useful for customizing configuration files or other scripts based on the notebook's runtime state, such as updating model configuration files to reflect the number of classes dynamically determined from the dataset."],"metadata":{"id":"ik8c7w86BOif"}},{"cell_type":"code","source":["#customize iPython writefile\n","@register_line_cell_magic\n","def writetemplate(line, cell):\n","    with open(line, 'w') as f:\n","        f.write(cell.format(**globals()))"],"metadata":{"id":"X6rD01cOiaF3","executionInfo":{"status":"ok","timestamp":1707263630995,"user_tz":480,"elapsed":2,"user":{"displayName":"Sunidhi Ashtekar","userId":"09183025015823308917"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## **Customizing YOLOv5 Model Configuration for Road Sign Detection**\n","\n","This cell dynamically generates a custom YOLOv5 model configuration file named custom_yolov5s.yaml. This file is tailored specifically for the road sign detection project, taking into account the unique aspects of the dataset, such as the number of classes. By creating a custom model configuration, we ensure that the YOLOv5 model is optimally adjusted for our specific object detection task, potentially enhancing its effectiveness and efficiency.\n","\n","- **Number of Classes (nc):** Set dynamically to match the number of road sign categories identified in the dataset.\n","\n","- **Depth and Width Multiples:** Adjustments to the model's depth and width can fine-tune its size and computational complexity, affecting both accuracy and performance.\n","\n","- **Anchors:** Predefined anchor box sizes that the model will use to detect objects. These are typically optimized for the scale and aspect ratios of objects in the training dataset.\n","\n","- **Model Architecture:** The backbone and head of the YOLOv5 model are defined, detailing the layers and their configurations."],"metadata":{"id":"QeM3qbSyBqH1"}},{"cell_type":"code","source":["%%writetemplate /content/yolov5/models/custom_yolov5s.yaml\n","\n","# parameters\n","nc: {num_classes}  # number of classes\n","depth_multiple: 0.33  # model depth multiple\n","width_multiple: 0.50  # layer channel multiple\n","\n","# anchors\n","anchors:\n","  - [10,13, 16,30, 33,23]  # P3/8\n","  - [30,61, 62,45, 59,119]  # P4/16\n","  - [116,90, 156,198, 373,326]  # P5/32\n","\n","# YOLOv5 backbone\n","backbone:\n","  # [from, number, module, args]\n","  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n","   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n","   [-1, 3, BottleneckCSP, [128]],\n","   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n","   [-1, 9, BottleneckCSP, [256]],\n","   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n","   [-1, 9, BottleneckCSP, [512]],\n","   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n","   [-1, 1, SPP, [1024, [5, 9, 13]]],\n","   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n","  ]\n","\n","# YOLOv5 head\n","head:\n","  [[-1, 1, Conv, [512, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n","   [-1, 3, BottleneckCSP, [512, False]],  # 13\n","\n","   [-1, 1, Conv, [256, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n","   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n","\n","   [-1, 1, Conv, [256, 3, 2]],\n","   [[-1, 14], 1, Concat, [1]],  # cat head P4\n","   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n","\n","   [-1, 1, Conv, [512, 3, 2]],\n","   [[-1, 10], 1, Concat, [1]],  # cat head P5\n","   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n","\n","   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n","  ]"],"metadata":{"id":"t0jRsoghid0n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Training YOLOv5 on Custom Road Sign Dataset**\n","\n","In this section, we use the custom YOLOv5 configuration file tailored for our dataset, ensuring that the model architecture aligns with the specifics of our task, such as the number of classes."],"metadata":{"id":"uiO0j5j_Crb2"}},{"cell_type":"code","source":["# train yolov5s on custom data for 100 epochs\n","# time its performance\n","%%time\n","%cd /content/yolov5/\n","!python train.py --img 416 --batch 16 --epochs 80 --data {dataset.location}/data.yaml --cfg ./models/custom_yolov5s.yaml --weights '' --name yolov5s_results  --cache\n"],"metadata":{"id":"3F-O94UWjwR0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Navigating to the Training Output Directory**"],"metadata":{"id":"jB20rmBUC44e"}},{"cell_type":"code","source":["%cd runs/train/yolov5s_results\n","%cd /content/yolov5/runs/train/yolov5s_results/"],"metadata":{"id":"gVQLhrZo-s1p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## **Visualizing Training Results**\n","\n","YOLOv5 automatically generates a results.png file that plots various training metrics over time, including precision, recall, and loss values. This visual representation helps in understanding how the model improved and stabilized over the training epochs."],"metadata":{"id":"i6q0EqTZDUI7"}},{"cell_type":"code","source":["# plot results.txt as results.png\n","Image(filename='/content/yolov5/runs/train/yolov5s_results/results.png', width=1000)  # view results.png"],"metadata":{"id":"HgOqI8tOktIu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Reviewing Validation Batch Labels**\n","\n","After the model has been trained, it's beneficial to visually inspect the validation results to understand how well the model is performing on unseen data. YOLOv5 generates images from the validation set with predicted bounding boxes and labels overlaid on the original images. This provides a qualitative assessment of the model's detection capabilities."],"metadata":{"id":"eFcWPHjNDUzq"}},{"cell_type":"code","source":["Image(filename='/content/yolov5/runs/train/yolov5s_results/val_batch0_labels.jpg', width=900)"],"metadata":{"id":"Srj4D-eDkt66"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Displaying Augmented Training Data Example**\n","\n","To gain insight into the training process and how the data is being presented to the model, it's useful to examine examples of the augmented training data."],"metadata":{"id":"439HMEuDD8jH"}},{"cell_type":"code","source":["# print out an augmented training example\n","print(\"GROUND TRUTH AUGMENTED TRAINING DATA:\")\n","Image(filename='/content/yolov5/runs/train/yolov5s_results/train_batch0.jpg', width=900)"],"metadata":{"id":"t62AX4U4k3wS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Locating Trained Model Weights**"],"metadata":{"id":"0p_-j8LhEHoz"}},{"cell_type":"code","source":["# trained weights are saved by default in our weights folder\n","%ls /content/yolov5/runs/"],"metadata":{"id":"gtl1OkWXlB7K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Running Inference with Trained YOLOv5 Model**\n","\n","After training the YOLOv5 model on the road sign dataset and identifying the location of the best performing weights (best.pt), the next step is to use these weights to run inference on new data."],"metadata":{"id":"PGChjt2xEKNB"}},{"cell_type":"code","source":["%cd /content/yolov5/\n","!python detect.py --weights runs/train/yolov5s_results/weights/best.pt --img 416 --conf 0.4 --source /content/yolov5/Road-Sign-Detection-in-Real-Time-3/test/images/\n"],"metadata":{"id":"hWMt4IfIlFdC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Displaying Inference Results**\n","\n","After running inference on your test dataset, you might have a collection of images with detected objects highlighted by bounding boxes."],"metadata":{"id":"XjFMVKZeEz6P"}},{"cell_type":"code","source":["for imageName in glob.glob('/content/yolov5/runs/detect/exp2/*.jpg')[:10]:\n","    display(Image(filename=imageName))"],"metadata":{"id":"bC6U-hJwlTO6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## **Saving Trained Model Weights to Google Drive**"],"metadata":{"id":"-VIOMmeWFBhS"}},{"cell_type":"code","source":["drive.mount('/content/gdrive')\n","%cp /content/yolov5/runs/train/yolov5s_results/weights/last.pt /content/gdrive/My\\ Drive"],"metadata":{"id":"fwtoDF6TNm-a"},"execution_count":null,"outputs":[]}]}